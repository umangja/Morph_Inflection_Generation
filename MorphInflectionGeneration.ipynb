{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MorphInflectionGeneration.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "2G9jw6XVsRvq",
        "LVxgyrYyswc8",
        "X7qVwwbQtRCk",
        "7Lc7R-jGtfL2",
        "X1Jf4tqCtx0o",
        "HTpinhgStz74",
        "FFLrEI0i9FVQ",
        "znHLwxdu9arJ",
        "GVJzOHfJ9kdI",
        "MEi6yXVB6thb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umangja/Morph_Inflection_Generation/blob/master/MorphInflectionGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G9jw6XVsRvq",
        "colab_type": "text"
      },
      "source": [
        "# **Importing important library**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhN7DldiPdZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch \n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import math\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVxgyrYyswc8",
        "colab_type": "text"
      },
      "source": [
        "# Set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92dg3g3wsMbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7qVwwbQtRCk",
        "colab_type": "text"
      },
      "source": [
        "# Processing file and other helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7ALjkmWtY_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"**Processing Files And Other Useful Function**\"\"\"\n",
        "\n",
        "def file_preprocess(file_name):\n",
        "    \n",
        "    # from google.colab import files\n",
        "    # uploaded = files.upload()\n",
        "    \n",
        "    pandas_holder = pd.read_csv(file_name, sep='\\t', names=['word', 'inflection', 'feature'])\n",
        "    \n",
        "    words_list = [list(word) for word in list(pandas_holder['word'])]\n",
        "    inflection_list = [list(inflection) for inflection in list(pandas_holder['inflection'])]\n",
        "    feature_list = [feat.split(';') for feat in list(pandas_holder['feature'])]\n",
        "    #print(words_list);\n",
        "\n",
        "    vocab_chars_word = [char for word in words_list for char in word]\n",
        "    vocab_chars_inflection = [char for word in inflection_list for char in word]\n",
        "    # print(vocab_chars_word)\n",
        "\n",
        "    char_vocab = ['<pad>', '<sos>', '<eos>'] + list(set(vocab_chars_word+vocab_chars_inflection))\n",
        "    feat_vocab = ['<pad>'] + list(set([f for feat in feature_list for f in feat]))\n",
        "\n",
        "    # print(char_vocab)\n",
        "    char_to_index = dict(zip(char_vocab, list(range(len(char_vocab)))))\n",
        "    index_to_char = dict(zip(list(range(len(char_vocab))), char_vocab))\n",
        "    feature_to_index = dict(zip(feat_vocab, list(range(len(feat_vocab)))))\n",
        "\n",
        "    # 2 added here b/c sos and eos appended to each word and inflection word\n",
        "    word_list_length = [len(word)+2 for word in words_list]\n",
        "    inflection_list_length = [len(word)+2 for word in inflection_list]\n",
        "    feature_list_length = [len(feat) for feat in feature_list]\n",
        "\n",
        "    max_word_length = max(word_list_length)\n",
        "    max_inflection_length = max(inflection_list_length)\n",
        "    max_feature_length = max(feature_list_length)\n",
        "\n",
        "    return words_list, inflection_list, feature_list, char_to_index, index_to_char, feature_to_index, max_word_length, max_inflection_length, max_feature_length\n",
        "\n",
        "def boundary_appender(sentence_list):\n",
        "    sent_list = []\n",
        "    for sentence in sentence_list:\n",
        "        sentence.append('<eos>')\n",
        "        sentence.insert(0,'<sos>')\n",
        "        sent_list.append(sentence)\n",
        "    return sent_list\n",
        "\n",
        "def sentence_pad(sentence_list, length):\n",
        "    sent_list = []\n",
        "    for sentence in sentence_list:\n",
        "        while len(sentence) != length:\n",
        "            sentence.append(char_to_index['<pad>'])\n",
        "        sent_list.append(sentence)\n",
        "    return sent_list\n",
        "\n",
        "def word_pad(word, length):\n",
        "    while len(word) != length:\n",
        "        word.append(char_to_index['<pad>'])\n",
        "    return word\n",
        "\n",
        "def source_taget_equal_length_maker(source, target):    \n",
        "    source_lengths = [len(s) for s in source]\n",
        "    target_lengths = [len(t) for t in target]\n",
        "    max_lengths = max(max(source_lengths), max(target_lengths))\n",
        "\n",
        "    source_sent = sentence_pad(source, max_lengths)\n",
        "    target_sent = sentence_pad(target, max_lengths)\n",
        "    return source_sent, target_sent, max_lengths\n",
        "\n",
        "def sentence_encoder(sentence_list, dictionary):\n",
        "    sent_list = [[dictionary[word] for word in sentence]for sentence in sentence_list]\n",
        "    return sent_list\n",
        "\n",
        "def mini_batch_creator(sentence_list, batch_size):\n",
        "    final = [sentence_list[i * batch_size:(i + 1) * batch_size] for i in range((len(sentence_list) + batch_size - 1) // batch_size)]\n",
        "    return final\n",
        "  \n",
        "def ploatter(loss, acc, val_loss, val_acc):\n",
        "    fig = plt.figure()\n",
        "\n",
        "    # x axis values\n",
        "    x = list(range(1,len(loss)+1))\n",
        "    fig1=plt.figure(figsize=(18, 12), dpi= 80, facecolor='w', edgecolor='k')\n",
        "    fig1=plt.subplot(1,2,1)\n",
        "\n",
        "    lines_1 = plt.plot(x, loss, x, val_loss)\n",
        "    l1, l2 = lines_1\n",
        "    plt.setp(lines_1, linestyle='--')\n",
        "    plt.setp(l1, linewidth=2, color='b', label='Train Loss')\n",
        "    plt.setp(l2, linewidth=2, color='g', label='Validation Loss')\n",
        "\n",
        "    plt.legend(loc='upper right')\n",
        "    fig1.set_title('Loss/Epoch')\n",
        "    fig1.set_xlabel('Epoch')\n",
        "    fig1.set_ylabel('loss')\n",
        "\n",
        "    fig2=plt.subplot(1, 2, 2)\n",
        "\n",
        "    lines_2 = plt.plot(x, acc, x, val_acc)\n",
        "    l1, l2 = lines_2\n",
        "    plt.setp(lines_2, linestyle='--')\n",
        "    plt.setp(l1, linewidth=2, color='b', label='Train Accuracy')\n",
        "    plt.setp(l2, linewidth=2, color='g', label='Validation Accuracy')\n",
        "\n",
        "    plt.legend(loc='upper right')\n",
        "    fig2.set_title('Accuracy/Epoch')\n",
        "    fig2.set_xlabel('Epoch')\n",
        "    fig2.set_ylabel('Accuracy')\n",
        "    \n",
        "    \n",
        "    fig.savefig(file_name+'.png')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lc7R-jGtfL2",
        "colab_type": "text"
      },
      "source": [
        "# Encoder Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAWRCOHLtjCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,vocab_size,embedding_size, encoder_hid_dem,decoder_hid_dem,bidirectional,dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder_hid_dem  = encoder_hid_dem\n",
        "        self.encoder_n_direction=1;\n",
        "        self.bias = False\n",
        "        self.dropout=dropout \n",
        "        if(bidirectional==True):\n",
        "            self.encoder_n_direction=2;\n",
        "\n",
        "        self.embedding_layer  = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, padding_idx=0)\n",
        "        self.GRU_layer        = nn.GRU(input_size=embedding_size, hidden_size=encoder_hid_dem, batch_first=True, bidirectional=bidirectional)\n",
        "        self.fc               = nn.Linear(encoder_hid_dem*self.encoder_n_direction,decoder_hid_dem)\n",
        "        self.dropout          = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, input_word):\n",
        "        # print(input_word.size())\n",
        "        #[batch_size    src_sent_lent]\n",
        "\n",
        "        embed_out = self.embedding_layer(input_word)\n",
        "        #[BATCH_SIZE    src_sent_lent   embedding_dim]\n",
        "        \n",
        "        embed_out = F.relu(embed_out)\n",
        "        embed_out = self.dropout(embed_out)\n",
        "\n",
        "        self.batch = embed_out.size()[0]\n",
        "\n",
        "        # hidden =  self.init_hidden()\n",
        "        GRU_out,hidden = self.GRU_layer(embed_out)\n",
        "\n",
        "        \n",
        "        # print(GRU_out.size())\n",
        "        # print(hidd.size())\n",
        "\n",
        "        #[BATCH_SIZE    sec_sent_len    n_direction*hid_dem]\n",
        "        #[n_layer*n_direction   batch_size    hid_dem]\n",
        "\n",
        "        #where the first hid_dim elements in the third axis are the hidden states from the top layer forward RNN, and the last hid_dim elements are hidden states from the top layer backward RNN\n",
        "\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "\n",
        "        GRU_out = F.relu(GRU_out)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))\n",
        "        \n",
        "        # print(GRU_out.size())\n",
        "        # print(hidden.size())\n",
        "\n",
        "        #outputs = [batch_size    src sent len, encoder_hid_dim * n_direction]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        return GRU_out,hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (Variable(torch.eye(1, self.encoder_hid_dem)).unsqueeze(1).repeat(2, self.batch, 1).to(self.device))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1Jf4tqCtx0o",
        "colab_type": "text"
      },
      "source": [
        "# Attention Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJRlvs72t3AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,encoder_hid_dem,decoder_hid_dem,bidirectional):\n",
        "        super().__init__()\n",
        "        self.enc_hid_dim = encoder_hid_dem\n",
        "        self.dec_hid_dim = decoder_hid_dem\n",
        "        self.encoder_n_direction=1;\n",
        "        if(bidirectional==True):\n",
        "            self.encoder_n_direction=2;\n",
        "            \n",
        "        self.attn = nn.Linear((encoder_hid_dem * self.encoder_n_direction) + decoder_hid_dem, decoder_hid_dem)\n",
        "        self.v = nn.Parameter(torch.rand(decoder_hid_dem))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [batch_size    ,src sent len, enc hid dim * encoder_n_direction]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len    = encoder_outputs.shape[1]\n",
        "        \n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        #hidden          = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * encoder_n_direction]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2))) \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        #attention= [batch size, src len]\n",
        "            \n",
        "        return F.softmax(attention, dim=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTpinhgStz74",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "346f9Wf2uA2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, decoder_hid_dem, encoder_hid_dem, vocab_size,embedding_dim,attention,decoder_input_size,linear_input_size,bidirectional,dropout):\n",
        "        super().__init__()      \n",
        "        self.encoder_hid_dem=encoder_hid_dem\n",
        "        self.decoder_hid_dem=decoder_hid_dem\n",
        "        self.attention=attention\n",
        "        self.dropout = dropout\n",
        "        self.output_dim = vocab_size\n",
        "\n",
        "        self.decoder_n_direction=1;\n",
        "        if(bidirectional==True):\n",
        "            self.decoder_n_direction=2;\n",
        "            \n",
        "        self.GRU_layer_out = nn.GRU(decoder_input_size,decoder_hid_dem)\n",
        "        self.out_layer = nn.Linear(in_features=linear_input_size, out_features=vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        #self.GRU_layer_out.bias = torch.nn.Parameter(torch.zeros(decoder_input_size))\n",
        "\n",
        "    def forward(self, feature, hidden,actual_word,encoder_outputs):\n",
        "\n",
        "        feature = feature.unsqueeze(1)\n",
        "        # print('decoder')\n",
        "        # print(feature.size())\n",
        "        #[batch_size    src_sent_lent=1   feat_size=6]\n",
        "       \n",
        "        # print(hidden.size())\n",
        "        # [batch_size     dec_hid_dim]\n",
        "\n",
        "\n",
        "        # print(actual_word.size())\n",
        "        # [batch_size    src_sent_lent=1   embedding_dim]\n",
        "\n",
        "        # print(encoder_outputs.size())\n",
        "        # outputs = [batch_size    src sent len, encoder_hid_dim * encoder_n_directional]\n",
        "\n",
        "\n",
        "        a = self.attention(hidden,encoder_outputs)\n",
        "        #  print(a.size())\n",
        "        # [batch_size    src_sent_len]\n",
        "\n",
        "        a = a.unsqueeze(1)\n",
        "        #a = [batch size, 1, src len] \n",
        "\n",
        "        weighted = torch.bmm(a,encoder_outputs)\n",
        "        # print(weighted.size())\n",
        "        # weighted = [batch size, 1, enc_hid_dim * encoder_n_direction]\n",
        "        # if len(actual_word.size()) != 0:\n",
        "        input_char = torch.cat((actual_word,feature,weighted),2) \n",
        "        # else:\n",
        "        #     input_char = torch.cat((feature,weighted),2)\n",
        "\n",
        "        input_char=input_char.permute(1,0,2)\n",
        "        #  print(input_char.size())\n",
        "        # [1    BATCH_SIZE      decoder_input_size]\n",
        "\n",
        "        hidden = hidden.unsqueeze(0)\n",
        "        # print(hidden.size())\n",
        "        #[1 batch_size decoder_hid_dem]\n",
        "       \n",
        "        output, hidden = self.GRU_layer_out(input_char, hidden)\n",
        "\n",
        "        # print(output.size())\n",
        "        # [sent_len=1   batch_size  decoder_n_direction*decoder_hid_dem]\n",
        "        # print(hidden.size())\n",
        "        # [n_layer*n_direction    BATCH_SIZE      hid_dem]\n",
        "\n",
        "\n",
        "\n",
        "        output = F.leaky_relu(output)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output = torch.cat((output.squeeze(0),weighted.squeeze(1),actual_word.squeeze(1)),dim=1)\n",
        "        pre_out = self.out_layer(output)\n",
        "        predicted_output = F.log_softmax(pre_out, dim=1)\n",
        "\n",
        "        # print(predicted_output.size())\n",
        "        # [ batch_size vacab_size ]\n",
        "        return predicted_output, hidden.squeeze(0)  \n",
        "\n",
        "    def init_hidden(self, batch):\n",
        "        return (Variable(torch.eye(1, self.decoder_hid_dem)).unsqueeze(1).repeat(1, batch, 1).to(self.device),Variable(torch.eye(1, self.decoder_hid_dem)).unsqueeze(1).repeat(1, batch, 1).to(self.device))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKl6sFkWvfLk",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZUW-inydHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self,input_word,output_word,features_word,teaching_forcing_ratio,limit):\n",
        "        #print(input_word)\n",
        "        #print(input_word.size())\n",
        "        input_word = input_word.to(self.device)\n",
        "        output_word = output_word.to(self.device)\n",
        "        features_word = features_word.to(self.device)\n",
        "        \n",
        "        batch_size= input_word.size()[0]\n",
        "        if(limit==0):\n",
        "            max_len   = input_word.size()[1]\n",
        "        else:\n",
        "            max_len   = limit\n",
        "        vocabsize = self.decoder.output_dim\n",
        "        \n",
        "        actual_word = self.encoder.embedding_layer(torch.tensor(char_to_index['<sos>']).view(1, -1).to(self.device)).repeat(batch_size, 1, 1)\n",
        "        encoder_outputs,hidden = self.encoder(input_word)\n",
        "        features=features_word[:,:]\n",
        "        \n",
        "        predicted_word = torch.zeros(max_len,batch_size,vocabsize).to(self.device)\n",
        "        \n",
        "        for t in range(1,max_len):\n",
        "            output,hidden=self.decoder(features, hidden,actual_word,encoder_outputs)\n",
        "            #print(output.size())\n",
        "            predicted_word[t] = output \n",
        "            topv, topi = output.topk(1)\n",
        "            bs = topi.size()[0]\n",
        "            temp2 = torch.zeros(0,1,300).to(self.device)\n",
        "            for row in range(bs):\n",
        "                index = topi[row][0].item()\n",
        "                temp = self.encoder.embedding_layer(torch.tensor(index).view(1, -1).to(self.device))\n",
        "                temp2 = torch.cat((temp2,temp))\n",
        "\n",
        "            teacher_force = random.random() < teaching_forcing_ratio\n",
        "            if teacher_force == 1:\n",
        "                actual_word = self.encoder.embedding_layer(output_word[:,t]).unsqueeze(1)\n",
        "            else:\n",
        "                actual_word = temp2\n",
        "                \n",
        "        return predicted_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNgydzETvRnK",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qcJITFkvWhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = 'hindi-train-medium'\n",
        "evaluate_file = 'hindi-dev'\n",
        "\n",
        "words_list, inflection_list, feature_list, char_to_index, index_to_char, feature_to_index, max_word_length, max_inflection_length, max_feature_length = file_preprocess(file_name)\n",
        "\n",
        "words_list      = boundary_appender(words_list)\n",
        "inflection_list = boundary_appender(inflection_list)\n",
        "\n",
        "word_encode_list  = sentence_encoder(words_list, char_to_index)\n",
        "inf_encode_list   = sentence_encoder(inflection_list, char_to_index)\n",
        "feat_encode_list  = sentence_pad(sentence_encoder(feature_list, feature_to_index), max_feature_length)\n",
        "\n",
        "\n",
        "test_words_list, test_inflection_list, test_feature_list, _, _, _, _, _, _ = file_preprocess(evaluate_file)\n",
        "t_word_list       = boundary_appender(test_words_list)\n",
        "t_inflection_list = boundary_appender(test_inflection_list)\n",
        "\n",
        "test_word_encode_list = sentence_encoder(t_word_list, char_to_index)\n",
        "test_inf_encode_list  = sentence_encoder(t_inflection_list, char_to_index)\n",
        "test_feat_encode_list = sentence_pad(sentence_encoder(test_feature_list, feature_to_index), max_feature_length)\n",
        "\n",
        "\n",
        "\"\"\"**Hyperparameters**\"\"\"\n",
        "embedding_size   = 300\n",
        "encoder_hid_dem  = 300\n",
        "decoder_hid_dem  = 3*encoder_hid_dem\n",
        "vocab_size       = len(char_to_index.keys())\n",
        "dropout          = .75\n",
        "BATCH_SIZE       = 10         \n",
        "EPOCH_SIZE       = 30\n",
        "learning_rate    = 0.00069\n",
        "encoder_bidirectional =True\n",
        "decoder_birectional   =True\n",
        "teacher_forcing_ratio = .5\n",
        "\n",
        "\n",
        "# embedding_size   = 300\n",
        "# encoder_hid_dem  = 300\n",
        "# decoder_hid_dem  = 1*encoder_hid_dem\n",
        "# vocab_size       = len(char_to_index.keys())\n",
        "# dropout          = .75\n",
        "# BATCH_SIZE       = 10         \n",
        "# EPOCH_SIZE       = 10\n",
        "# learning_rate    = 0.00069\n",
        "# encoder_bidirectional =True\n",
        "# decoder_birectional   =True\n",
        "# teacher_forcing_ratio = .5\n",
        "# dropout         = 0.5\n",
        "\n",
        "\n",
        "if(encoder_bidirectional):\n",
        "    decoder_input_size  =len(feat_encode_list[0]) + embedding_size+(encoder_hid_dem)*2\n",
        "    linear_input_size=embedding_size+decoder_hid_dem+(encoder_hid_dem)*2\n",
        "else:\n",
        "    decoder_input_size  =len(feat_encode_list[0]) + embedding_size+(encoder_hid_dem)*1\n",
        "    linear_input_size=embedding_size+decoder_hid_dem+(encoder_hid_dem)*1\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh4v3DfP5Scb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention = Attention(encoder_hid_dem,decoder_hid_dem,encoder_bidirectional)\n",
        "enc       = Encoder(vocab_size,embedding_size,encoder_hid_dem,decoder_hid_dem,encoder_bidirectional,dropout)\n",
        "dec       = Decoder(decoder_hid_dem,encoder_hid_dem,vocab_size,embedding_size,attention,decoder_input_size,linear_input_size,decoder_birectional,dropout)\n",
        "\n",
        "model     = Seq2Seq(enc,dec,device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLrEI0i9FVQ",
        "colab_type": "text"
      },
      "source": [
        "# Intialization of weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEV1o5DA9OhQ",
        "colab_type": "code",
        "outputId": "ca5c57f3-0634-4f21-d19d-e328dfd7f683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding_layer): Embedding(58, 300, padding_idx=0)\n",
              "    (GRU_layer): GRU(300, 300, batch_first=True, bidirectional=True)\n",
              "    (fc): Linear(in_features=600, out_features=900, bias=True)\n",
              "    (dropout): Dropout(p=0.75)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=1500, out_features=900, bias=True)\n",
              "    )\n",
              "    (GRU_layer_out): GRU(906, 900)\n",
              "    (out_layer): Linear(in_features=1800, out_features=58, bias=True)\n",
              "    (dropout): Dropout(p=0.75)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znHLwxdu9arJ",
        "colab_type": "text"
      },
      "source": [
        "# Counting Total Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhDPcKYu9ZEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVJzOHfJ9kdI",
        "colab_type": "text"
      },
      "source": [
        "# Other Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk2We_zZ9jgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfrlrTwO-uSx",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K473tnyQ-wXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,input_word_minibatches,output_word_minibatches,feature_minibatches,optimizer,criterion,clip):\n",
        "    model.train()\n",
        "    epoch_loss=0\n",
        "    ct=0\n",
        "    for batch_idx, _ in enumerate(input_word_minibatches):\n",
        "        train_word = input_word_minibatches[batch_idx]\n",
        "        train_label = output_word_minibatches[batch_idx]\n",
        "        train_feat = feature_minibatches[batch_idx]\n",
        "\n",
        "        train_word, train_label, max_length = source_taget_equal_length_maker(train_word, train_label)\n",
        "\n",
        "        train_word = torch.tensor(train_word)\n",
        "        train_label = torch.tensor(train_label)\n",
        "        train_feat = torch.FloatTensor(train_feat)\n",
        "        \n",
        "        #print(train_word.size())\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predicted_word = model(train_word,train_label,train_feat,0.5,0)\n",
        "\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        train_label = train_label.permute(1,0)\n",
        "        predicted_word = predicted_word[1:].view(-1,predicted_word.shape[-1])\n",
        "        trg = train_label[1:].contiguous().view(-1).to(device)\n",
        "        #print(trg.size())\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(predicted_word, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        ct=ct+1\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / ct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEi6yXVB6thb",
        "colab_type": "text"
      },
      "source": [
        "# Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3rzBEZJ6tEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model,input_word_minibatches,output_word_minibatches,feature_minibatches,criterion):\n",
        "    model.eval()\n",
        "    epoch_loss=0\n",
        "    with torch.no_grad():\n",
        "      \n",
        "          train_word = torch.tensor(input_word_minibatches)\n",
        "          train_label = torch.tensor(output_word_minibatches)\n",
        "          train_feat = torch.FloatTensor(feature_minibatches)\n",
        "\n",
        "          predicted_word = model(train_word,train_label,train_feat,0,0)\n",
        "\n",
        "          #trg = [trg sent len, batch size]\n",
        "          #output = [trg sent len, batch size, output dim]\n",
        "          \n",
        "          train_label = output_word_minibatches.permute(1,0)\n",
        "          predicted_word = predicted_word[1:].view(-1,predicted_word.shape[-1])\n",
        "          trg = train_label[1:].contiguous().view(-1).to(device)\n",
        "\n",
        "          #predicted_word = predicted_word[:,1:]\n",
        "          #trg = output_word_minibatches[:,1:]\n",
        "\n",
        "          #trg = [(trg sent len - 1) * batch size]\n",
        "          #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "          loss = criterion(predicted_word, trg)\n",
        "          epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4OJDZ00buZw",
        "colab_type": "text"
      },
      "source": [
        "# Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2yziYdadkyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_strings(predicted,target):\n",
        "  topv,predicted = torch.topk(predicted,1,1)\n",
        "  predicted_string = \"\"\n",
        "  target_string    = \"\"\n",
        "  for character in range(predicted.size()[0]):\n",
        "      if index_to_char[predicted[character].item()] == '<eos>':\n",
        "          break;\n",
        "      predicted_string=predicted_string+index_to_char[predicted[character].item()]\n",
        "      \n",
        "  for character in range(target.size()[0]):\n",
        "      if index_to_char[target[character].item()] == '<eos>':\n",
        "          break;\n",
        "      target_string=target_string+index_to_char[target[character].item()]\n",
        "\n",
        "  return target_string,predicted_string\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogiJ_8jJbw6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,test_words,test_inflection,test_feat,criterion):\n",
        "    model.eval()\n",
        "    epoch_loss=0\n",
        "    with torch.no_grad():\n",
        "      \n",
        "          test_words      = torch.tensor(test_words)\n",
        "          test_label      = torch.tensor(test_inflection)\n",
        "          test_feat       = torch.FloatTensor(test_feat)\n",
        "\n",
        "          predicted_word = model(test_words,test_label,test_feat,0,50)\n",
        "          predicted_word = predicted_word[1:].view(-1,predicted_word.shape[-1])\n",
        " \n",
        "          test_label = test_label.permute(1,0)\n",
        "          test_label = torch.cat([test_label,torch.zeros([max(0,50-test_label.size()[0]),1],dtype=torch.long)],0)\n",
        "\n",
        "          for index in range(test_label.size()[0],50):\n",
        "              test_label[index][0]=char_to_index['pad']\n",
        "          \n",
        "          #print(test_label.size())\n",
        "          trg = test_label[1:].contiguous().view(-1).to(device)\n",
        "          \n",
        "          #trg = [(trg sent len - 1) * batch size]\n",
        "          #output = [(trg sent len - 1) * batch size, output dim]\n",
        "          \n",
        "          loss = criterion(predicted_word, trg)\n",
        "          epoch_loss += loss.item()\n",
        "          \n",
        "          actual_string,predicted_string = to_strings(predicted_word,trg)\n",
        "          print(actual_string)\n",
        "          print(predicted_string)\n",
        "          print(epoch_loss)\n",
        "          acc=0\n",
        "          index=0\n",
        "          for charac in predicted_string:\n",
        "              if index>=len(actual_string):\n",
        "                break\n",
        "              if(charac==actual_string[index]):\n",
        "                acc=acc+1\n",
        "              index=index+1;\n",
        "              \n",
        "          acc=acc/len(actual_string)\n",
        "          print(acc)\n",
        "          \n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T6m4rYJvETX",
        "colab_type": "text"
      },
      "source": [
        "# Actual Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gcKpcxcDEKMg",
        "colab": {}
      },
      "source": [
        "\n",
        "input_word_minibatches  = mini_batch_creator(word_encode_list, batch_size=BATCH_SIZE)\n",
        "output_word_minibatches = mini_batch_creator(inf_encode_list, batch_size=BATCH_SIZE)\n",
        "feature_minibatches     = mini_batch_creator(feat_encode_list, batch_size=BATCH_SIZE)\n",
        "\n",
        "# valid_word_minibatches  = mini_batch_creator(valid_word,batch_size=10)\n",
        "# valid_label_minibatches = mini_batch_creator(valid_label,batch_size=10)\n",
        "# valid_feat_minibatches  = mini_batch_creator(valid_feat,batch_size=10)\n",
        "\n",
        "valid_word, valid_label, max_length_1 = source_taget_equal_length_maker(test_word_encode_list, test_inf_encode_list)\n",
        "valid_word = torch.tensor(valid_word)\n",
        "valid_label = torch.tensor(valid_label)\n",
        "valid_feat = torch.FloatTensor(test_feat_encode_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKRdFHbvENVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Training():\n",
        "  CLIP = 1\n",
        "  best_valid_loss = float('inf')\n",
        "  for epoch in range(EPOCH_SIZE):\n",
        "      start_time = time.time()\n",
        "\n",
        "      train_loss = train(model, input_word_minibatches,output_word_minibatches,feature_minibatches, optimizer, criterion, CLIP)\n",
        "      valid_loss = validate(model, valid_word,valid_label,valid_feat, criterion)\n",
        "\n",
        "      end_time = time.time()\n",
        "\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gepi_hfOvywj",
        "colab_type": "text"
      },
      "source": [
        "# Printing Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PHUZkJuv3Ic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_file = 'hindi-test'\n",
        "test_words_list, test_inflection_lists, test_feature_list, _, _, _, _, _, _ = file_preprocess(test_file)\n",
        "\n",
        "test_words = boundary_appender(test_words_list)\n",
        "test_words = sentence_encoder(test_words, char_to_index)\n",
        "\n",
        "test_inflection = boundary_appender(test_inflection_lists)\n",
        "test_inflection = sentence_encoder(test_inflection, char_to_index)\n",
        "\n",
        "test_feat  = sentence_pad(sentence_encoder(test_feature_list, feature_to_index), max_feature_length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6dTaBzabZau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Testing():\n",
        "  count=0\n",
        "  for index in range(len(test_words)):\n",
        "      print(index)\n",
        "      test_word1 = [test_words[index]]\n",
        "      test_feat1 = [test_feat[index]]\n",
        "      test_label1 = [test_inflection[index]]\n",
        "\n",
        "      test_word1, test_label1, max_len = source_taget_equal_length_maker(test_word1,test_label1)\n",
        "      acc = test(model,test_word1,test_label1,test_feat1,criterion)\n",
        "      if acc==1:\n",
        "          count=count+1\n",
        "      print()\n",
        "  print('exactly correct no of examples are ',count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2k9-MxsOK5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Testing1():\n",
        "  count=0\n",
        "  model.load_state_dict(torch.load('model.pt'))\n",
        "  for index in range(len(test_words)):\n",
        "      print(index)\n",
        "      test_word1 = [test_words[index]]\n",
        "      test_feat1 = [test_feat[index]]\n",
        "      test_label1 = [test_inflection[index]]\n",
        "\n",
        "      test_word1, test_label1, max_len = source_taget_equal_length_maker(test_word1,test_label1)\n",
        "      acc = test(model,test_word1,test_label1,test_feat1,criterion)\n",
        "      if acc==1:\n",
        "          count=count+1\n",
        "      print()\n",
        "  print('exactly correct no of examples are ',count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5c6qV4-vqLM",
        "colab_type": "text"
      },
      "source": [
        "#Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAA808ZcvvXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPaNOF_fOPXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Testing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCYHEhSUOPZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Testing1()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}